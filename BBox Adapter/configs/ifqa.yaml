task: StrategyQA
train_ratio: 0.5
seed: 42
# MODEL
generator_model: meta-llama/Llama-3.2-1B-Instruct
critic_model: microsoft/deberta-v3-base
add_special_tokens: True
# MODE
critic_mode: classification # generation / classification
score_mode: # None / sum_logits / log_prob / neg_ppl / mean_logits
# SEARCH PARAMS
beam_size: 3
num_candidates: 3
max_length: 10
early_stopping: True
only_eval_answers: False
# QUERY_PARAMS
temperature: 1.0
frequency_penalty: 0 
presence_penalty: 0 
stop: ["\n", "\n\n", "\n\n\n"]
max_tokens: 80
# TRAINING_PARAMS
## offline warmup
offline_warmup_path: # set this to empty is offline warmup is undesired 
num_epochs_offline_warmup: 

## blackbox warmup
use_blackbox_warmup: True
num_epochs_blackbox_warmup: 3
num_candidates_blackbox_warmup: 10

## online finetuning
num_online_finetuning_repeat: 1

num_epochs: 3
batch_size: 64 
gradient_accumulation_steps: 1 


l2_reg_coef: 1.
energy_temp: 5.
warmup_steps: 50
learning_rate: 5.E-6
min_lr: 5.E-6
T_lr: 1000

use_outcome_supervision: True
num_negatives_for_training: 2
qa_template: "Q: <Q>\n\nA: <A>"

# EVALUATION
eval_blackbox: True
eval_unfinetuned: False
num_eval_rounds: 2

log_with_wandb: True
