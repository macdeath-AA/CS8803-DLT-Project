task: ScienceQA
train_ratio: 0.9

seed: 42

# MODEL
generator_model: gpt-3.5-turbo

# bert-base-cased (0.11B)
# bert-large-cased (0.34B)

critic_model: microsoft/deberta-v3-large
add_special_tokens: True

# MODE
critic_mode: classification # generation / classification
score_mode: # None / sum_logits / log_prob / neg_ppl / mean_logits

# SEARCH PARAMS
beam_size: 3
num_candidates: 10
max_length: 8
early_stopping: True
only_eval_answers: False

# QUERY_PARAMS
temperature: 1.0
frequency_penalty: 0 
presence_penalty: 0 
stop: ["\n", "\n\n", "\n\n\n"]
max_tokens: 80

# TRAINING_PARAMS
## offline warmup
offline_warmup_path: # set this to empty is offline warmup is undesired 
num_epochs_offline_warmup: 

## blackbox warmup
use_blackbox_warmup: True
num_epochs_blackbox_warmup: 3
num_candidates_blackbox_warmup: 10

## online finetuning
num_online_finetuning_repeat: 1

num_epochs: 3
batch_size: 14
gradient_accumulation_steps: 3


l2_reg_coef: 1.
energy_temp: 5.
warmup_steps: 50
learning_rate: 5.E-6
min_lr: 5.E-6
T_lr: 1000

use_outcome_supervision: True
num_negatives_for_training: 2
qa_template: "Question: <Q>\n\nAnswer: <A>"


# EVALUATION
eval_blackbox: True
eval_unfinetuned: False
num_eval_rounds: 3

log_with_wandb: True
